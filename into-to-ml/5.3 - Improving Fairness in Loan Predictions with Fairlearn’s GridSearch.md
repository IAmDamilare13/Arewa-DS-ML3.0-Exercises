## Improving Fairness in Loan Predictions with Fairlearn’s GridSearch

### 1. Introduction to Fairlearn
Fairlearn is an open-source, community-driven toolkit aimed at helping data scientists assess and mitigate unfairness in AI systems [arXiv](https://arxiv.org/abs/2303.16626). It enables evaluation across demographic groups and provides multiple fairness-aware algorithms.

### 2. Notebook Overview
The notebook __“GridSearch with Census Data”__ (Fairlearn v0.6.2) demonstrates how to detect and reduce unfairness in a model using the Adult (Census) dataset. It rephrases an income prediction task (earning >$50K/year) as a loan approval problem.

### Steps Covered:
- __Data Loading & Preprocessing:__ Fetching the dataset, encoding labels, and preparing features.
- __Training Fairness-Unaware Predictor:__ Standard logistic regression is trained, ignoring fairness constraints.
- __Fairness Evaluation:__ The model is evaluated using demographic parity, measuring fairness in loan approval across sex groups.
- __Mitigation via GridSearch:__ The `GridSearch` reduction generates multiple predictors by optimizing trade-offs between accuracy and fairness. It outputs a set of Pareto-optimal models in the accuracy-disparity space.
- __Visual Analysis:__ Results are visualized as a scatter plot showing a Pareto front of models balancing accuracy and disparity.

### 3. Key Findings
- __Ignoring sensitive features isn't enough:__ Even after removing the ‘sex’ feature, the model still discriminated—males had much higher selection rates than females—highlighting the impact of correlated features.
- __Pareto-optimal trade-offs:__ The mitigation process yields models along a trade-off curve (Pareto front), allowing decision-makers to choose a model based on acceptable fairness-performance balance.
- __Model selection depends on context:__ In real-world scenarios, choosing the best model isn't just about minimizing disparity or maximizing accuracy—it must reflect business and ethical priorities.

### 4. Conclusions
- Fairlearn offers an effective approach to model fairness, combining assessment and mitigation within an accessible API.
- The notebook clearly illustrates that __model fairness requires more than simply removing sensitive attributes__; systemic biases can persist through correlated data.
- The __GridSearch reduction__ empowers practitioners to view and choose among models based on fairness–accuracy trade-offs.
- For an exemplary assignment, highlight not only the technical process but also reflect on practical implications—for example, societal impact and organizational priorities.
- An __adequate submission__ might describe the notebook’s workflow, but the inclusion of a discussion on these trade-offs and their interpretation elevates the work to exemplary.

Notebook: [GridSearch with Census Data — Fairlearn 0.13.0.dev0 documentation](https://fairlearn.org/main/auto_examples/plot_grid_search_census.html)