## Brief History of Large Language Models

This is a brief history on the evolution of large language models (LLMs)

![Image](https://github.com/IAmDamilare13/Arewa-DS-ML3.0-Exercises/blob/main/into-to-ml/llms_evolution.jpg)

| Year | Evolution | Comment |
|-------|---------|----------|
| 1966 | ELIZA | The first chabot build by human. It functions by creating an illusion of conversation by rephrasing user statements as questions. [Click here to visit ELIZA](https://web.njit.edu/~ronkowit/eliza.html)
| 1986 | Recurrent Neural Networks (RNN) | It was deeply inspired by the human brains and it interconnected neurons. Compared to previous networks with one direction flow, RNNs could remember previous inputs in their internal state through it feedback loop.
| 1997 | Long Short Term Memory (LSTM) | This is a specialized type of RNN with the ability to remember infomrmtion over long sequences, overcoming the short term memory limitation of RNNs.
| 2014 | Gated Recurrent Units (GRUs) | Designed to solve same problems of LSTM but with the simple and more streamline structure. It reduced logic gates makes it mor efficient in terms of computations.
| 2014 | Attention Mechanism | This birth a significant paradigm shift in sequence modelling with a fresh perspective. Unlike RNNs where performance decreases as sentences increases, attention allows the model to look back at the entire source selecting different parts based on relevance at each output step.
| 2017 | Transformers Architecture | This relies on attention mechanism to process sequences, and it multi-deah attention allowsa it to focus on different parts of sentences simultaneously capturing all contextual nuances at once. It laids the foundation for various LLMs.
| 2018 | Bidirectional Encoder Representations from Transformers (BERT), GPT | BERT considers left and right directions simultaneously unlike previous trasformers. Pretrianed on vast amount of text, it was the first foundational model fine-tuned for specific tasks setting new performance standards for various NLP benchmarks. Same year, OpenAI released GPT, the first Generative Pre-trained Tranformer
| 2020 | GPT-3 | It stunned the world with 175 bilion parameters, showing remarkable abilities in conversation, writing and coding
| 2022 onwards | | LLMs became mainstream, powering products like ChatGPT, Bard and Copilot 
    LLMS are still evolving rapidly and current research focuses on reducing biases and hallucinations, integrating reasoning and real-time knowledge retrieval, and moving towards multimodal AI that understands text, image, audio and video. Some are LaMa Falcon, GPT-4 LIMA, PaLM 2, Gemini, Dolly 2, Guanaco.


Source: [Brief History of Large Language Models & Generative AI](https://youtu.be/K7o5_Fj7_SY?si=tNHtcYDsuSjgr_Mb)
